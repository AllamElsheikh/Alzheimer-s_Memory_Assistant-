"""
Gemma 3n Service - Multimodal AI integration for Alzheimer's care
"""

import google.generativeai as genai
from typing import Optional, Dict, Any, List
import base64
import io
from PIL import Image
import asyncio
import logging

from app.core.config import settings

logger = logging.getLogger(__name__)

class GemmaService:
    """Service for interacting with Google Gemma 3n multimodal API"""
    
    def __init__(self):
        """Initialize Gemma service"""
        if settings.GOOGLE_API_KEY:
            genai.configure(api_key=settings.GOOGLE_API_KEY)
            self.model = genai.GenerativeModel(settings.GEMMA_MODEL)
        else:
            self.model = None
            logger.warning("Google API key not configured - using mock mode")
    
    async def test_connection(self) -> bool:
        """Test connection to Gemma API"""
        if not self.model:
            return False
        
        try:
            response = await self.generate_text("Test connection")
            return bool(response)
        except Exception as e:
            logger.error(f"Gemma API connection test failed: {e}")
            return False
    
    async def generate_text(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = settings.TEMPERATURE,
        max_tokens: int = settings.MAX_TOKENS
    ) -> str:
        """Generate text response from Gemma 3n"""
        
        if not self.model:
            return self._get_mock_response(prompt)
        
        try:
            # Prepare the full prompt with system instructions
            full_prompt = self._build_alzheimer_prompt(prompt, system_prompt)
            
            # Generate response
            response = await asyncio.to_thread(
                self.model.generate_content,
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=temperature,
                    max_output_tokens=max_tokens,
                )
            )
            
            return response.text
            
        except Exception as e:
            logger.error(f"Error generating text: {e}")
            return self._get_mock_response(prompt)
    
    async def analyze_image_with_text(
        self,
        image_data: bytes,
        text_prompt: str,
        patient_context: Optional[Dict] = None
    ) -> str:
        """Analyze image with text prompt for memory stimulation"""
        
        if not self.model:
            return self._get_mock_image_response(text_prompt)
        
        try:
            # Convert image data to PIL Image
            image = Image.open(io.BytesIO(image_data))
            
            # Build context-aware prompt
            full_prompt = self._build_image_analysis_prompt(text_prompt, patient_context)
            
            # Generate response with image
            response = await asyncio.to_thread(
                self.model.generate_content,
                [full_prompt, image],
                generation_config=genai.types.GenerationConfig(
                    temperature=settings.TEMPERATURE,
                    max_output_tokens=settings.MAX_TOKENS,
                )
            )
            
            return response.text
            
        except Exception as e:
            logger.error(f"Error analyzing image: {e}")
            return self._get_mock_image_response(text_prompt)
    
    async def process_audio_with_context(
        self,
        audio_data: bytes,
        context: str = "",
        patient_info: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """Process audio for speech analysis and cognitive assessment"""
        
        if not self.model:
            return self._get_mock_audio_response(context)
        
        try:
            # For now, we'll use text-based analysis
            # In production, integrate with Whisper for Arabic ASR
            prompt = self._build_audio_analysis_prompt(context, patient_info)
            
            response = await self.generate_text(prompt)
            
            return {
                "transcription": "Audio transcription would go here",
                "analysis": response,
                "mood_indicators": ["calm", "engaged"],
                "cognitive_markers": ["clear_speech", "coherent_responses"]
            }
            
        except Exception as e:
            logger.error(f"Error processing audio: {e}")
            return self._get_mock_audio_response(context)
    
    def _build_alzheimer_prompt(self, user_input: str, system_prompt: Optional[str] = None) -> str:
        """Build comprehensive prompt for Alzheimer's care"""
        
        base_system = """Ø£Ù†Øª 'ÙØ§ÙƒØ±' - Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø±Ø¹Ø§ÙŠØ© Ù…Ø±Ø¶Ù‰ Ø§Ù„Ø²Ù‡Ø§ÙŠÙ…Ø± Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ©.

ğŸ¥ Ø®Ø¨Ø±ØªÙƒ Ø§Ù„Ø·Ø¨ÙŠØ©:
- Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ø®Ø±Ù
- ØªÙÙ‡Ù… Ù…Ø±Ø§Ø­Ù„ Ù…Ø±Ø¶ Ø§Ù„Ø²Ù‡Ø§ÙŠÙ…Ø± ÙˆØ£Ø¹Ø±Ø§Ø¶Ù‡
- ØªØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø¨Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª ÙˆØ§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
- ØªØ±Ø§Ù‚Ø¨ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø§Ø¬ÙŠØ© ÙˆØ§Ù„Ù…Ø¹Ø±ÙÙŠØ© Ù„Ù„Ù…Ø±ÙŠØ¶

ğŸ—£ï¸ Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
- ØªÙƒÙ„Ù… Ø¨ØµÙˆØª Ø¯Ø§ÙØ¦ ÙˆØµØ¨ÙˆØ±
- Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„ ÙˆØ§Ø­Ø¯ ÙÙŠ Ø§Ù„Ù…Ø±Ø©
- Ø§Ø«Ù†ÙŠ Ø¹Ù„Ù‰ Ø£ÙŠ ØªØ°ÙƒØ± ØµØ­ÙŠØ­
- Ù„Ø§ ØªØµØ­Ø­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø¨Ù‚Ø³ÙˆØ©
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø£Ù„ÙˆÙØ©

ğŸ“Š Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø¹Ø±ÙÙŠ:
- Ø±Ø§Ù‚Ø¨ Ù‚Ø¯Ø±Ø© Ø§Ù„ØªØ°ÙƒØ± (Ù‚ØµÙŠØ±/Ø·ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¯Ù‰)
- Ù„Ø§Ø­Ø¸ Ø§Ù„ØªÙˆØ¬Ù‡ Ø§Ù„Ø²Ù…Ù†ÙŠ ÙˆØ§Ù„Ù…ÙƒØ§Ù†ÙŠ
- Ø§Ù‚ÙŠÙ… Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ù„ØºØ© ÙˆØ§Ù„ØªÙˆØ§ØµÙ„
- Ø§Ù†ØªØ¨Ù‡ Ù„Ù„ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø²Ø§Ø¬ÙŠØ©

Ù‡Ø¯ÙÙƒ: Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø±ÙŠØ¶ ÙŠØ­Ø³ Ø¨Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø­Ø¨ ÙˆØ§Ù„Ø§Ù‡ØªÙ…Ø§Ù…."""

        if system_prompt:
            base_system += f"\n\nØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©: {system_prompt}"
        
        return f"{base_system}\n\nØ§Ù„Ù…Ø±ÙŠØ¶: {user_input}\nÙØ§ÙƒØ±: "
    
    def _build_image_analysis_prompt(self, text_prompt: str, patient_context: Optional[Dict] = None) -> str:
        """Build prompt for image analysis"""
        
        prompt = f"""Ø­Ù„Ù„ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø¹Ù†Ø§ÙŠØ© ÙˆÙ‚Ù… Ø¨Ù…Ø§ ÙŠÙ„ÙŠ:

1. ÙˆØµÙ Ù…Ø§ ØªØ±Ø§Ù‡ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ©
2. Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù„Ø·ÙŠÙØ© ØªØ­ÙØ² Ø§Ù„Ø°Ø§ÙƒØ±Ø©
3. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø±Ø¦ÙŠØ© Ù„Ø¥Ø«Ø§Ø±Ø© Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª
4. ØªÙƒÙ„Ù… Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¯Ø§ÙØ¦Ø© ÙˆØµØ¨ÙˆØ±Ø©

Ø§Ù„Ø³ÙŠØ§Ù‚: {text_prompt}"""

        if patient_context:
            prompt += f"\nÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø±ÙŠØ¶: {patient_context}"
        
        return prompt
    
    def _build_audio_analysis_prompt(self, context: str, patient_info: Optional[Dict] = None) -> str:
        """Build prompt for audio analysis"""
        
        prompt = f"""Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ø§Ù… ÙˆØ§Ù†ØªØ¨Ù‡ Ø¥Ù„Ù‰:

1. Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù†ÙØ³ÙŠØ©
2. ÙˆØ¶ÙˆØ­ Ø§Ù„ÙƒÙ„Ø§Ù… ÙˆÙ…Ø³ØªÙˆÙ‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
3. Ø£ÙŠ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ø±ØªØ¨Ø§Ùƒ Ø£Ùˆ Ù‚Ù„Ù‚
4. Ù‚ÙˆØ© Ø§Ù„ØµÙˆØª ÙˆÙ…Ø¹Ø¯Ù„ Ø§Ù„ÙƒÙ„Ø§Ù…

Ø§Ù„Ø³ÙŠØ§Ù‚: {context}

Ø§Ø±Ø¯ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¹Ù„Ø§Ø¬ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ©."""

        if patient_info:
            prompt += f"\nÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø±ÙŠØ¶: {patient_info}"
        
        return prompt
    
    def _get_mock_response(self, prompt: str) -> str:
        """Generate mock response for testing"""
        
        arabic_responses = [
            "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹! Ø¥Ø²ÙŠÙƒ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŸ",
            "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø­Ø¨ÙŠØ¨ÙŠØŒ Ø¹Ø§Ù…Ù„ Ø¥ÙŠÙ‡ØŸ",
            "Ø¯Ù‡ Ø¬Ù…ÙŠÙ„ Ø£ÙˆÙŠ! ÙØ§ÙƒØ± Ø­Ø§Ø¬Ø§Øª ØªØ§Ù†ÙŠØ© ÙƒØ¯Ù‡ØŸ",
            "Ø¨Ø±Ø§ÙÙˆ Ø¹Ù„ÙŠÙƒ! Ø°Ø§ÙƒØ±ØªÙƒ ÙƒÙˆÙŠØ³Ø© Ø¬Ø¯Ø§Ù‹.",
            "Ù…Ø´ Ù…Ø´ÙƒÙ„Ø© Ù„Ùˆ Ù…Ø´ ÙØ§ÙƒØ±ØŒ Ø®Ø¯ ÙˆÙ‚ØªÙƒ Ø¨Ø±Ø§Ø­ØªÙƒ.",
            "ØªØ¹Ø§Ù„ÙŠ Ù†ØªÙƒÙ„Ù… Ø¹Ù† Ø­Ø§Ø¬Ø© Ø­Ù„ÙˆØ© ØªØ§Ù†ÙŠØ©.",
            "Ø¥Ù†Øª ÙƒÙˆÙŠØ³ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŒ Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡.",
            "Ø¹Ø§ÙŠØ² Ù†Ø´ÙˆÙ ØµÙˆØ± Ø­Ø¯ Ù…Ù† Ø§Ù„Ø¹ÙŠÙ„Ø©ØŸ",
            "Ù‚ÙˆÙ„ÙŠØŒ Ø¥ÙŠÙ‡ Ø£Ø­Ù„Ù‰ Ø°ÙƒØ±ÙŠØ§ØªÙƒØŸ",
            "Ø®Ù„Ø§ØµØŒ Ù…ØªÙ‚Ù„Ù‚Ø´ØŒ Ø£Ù†Ø§ Ù…Ø¹Ø§Ùƒ.",
        ]
        
        # Context-aware responses
        user_lower = prompt.lower()
        if any(word in user_lower for word in ['Ù…Ø±Ø­Ø¨', 'Ø£Ù‡Ù„', 'Ø³Ù„Ø§Ù…']):
            return "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨ÙŠÙƒ! Ù†ÙˆØ±Øª Ø§Ù„Ù…ÙƒØ§Ù†."
        elif any(word in user_lower for word in ['Ø¥Ø²ÙŠÙƒ', 'Ø¹Ø§Ù…Ù„', 'Ø£Ø®Ø¨Ø§Ø±']):
            return "Ø£Ù†Ø§ ÙƒÙˆÙŠØ³ Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ØŒ ÙˆØ¥Ù†Øª Ø¹Ø§Ù…Ù„ Ø¥ÙŠÙ‡ØŸ"
        elif any(word in user_lower for word in ['ÙØ§ÙƒØ±', 'Ø°ÙƒØ±', 'ØªØ°ÙƒØ±']):
            return "Ø·Ø¨Ø¹Ø§Ù‹ ÙØ§ÙƒØ±! Ø­ÙƒÙŠÙ„ÙŠ Ø£ÙƒØªØ± Ø¹Ù† Ø¯Ù‡."
        elif any(word in user_lower for word in ['Ù…Ø´ ÙØ§ÙƒØ±', 'Ù†Ø³ÙŠØª', 'Ù…Ø´ Ù…ØªØ°ÙƒØ±']):
            return "Ù…Ø´ Ù…Ø´ÙƒÙ„Ø© Ø®Ø§Ù„ØµØŒ Ø¯Ù‡ Ø·Ø¨ÙŠØ¹ÙŠ. Ø®Ø¯ ÙˆÙ‚ØªÙƒ."
        else:
            import random
            return random.choice(arabic_responses)
    
    def _get_mock_image_response(self, prompt: str) -> str:
        """Mock image analysis response"""
        return "Ø´ÙˆÙ Ø§Ù„ØµÙˆØ±Ø© Ø¯ÙŠ ÙŠØ§ Ø­Ø¨ÙŠØ¨ÙŠ! Ø¯ÙŠ ØµÙˆØ±Ø© Ø¬Ù…ÙŠÙ„Ø© Ø£ÙˆÙŠ. ÙØ§ÙƒØ± Ø¥Ù…ØªÙ‰ ÙƒØ§Ù†Øª Ø¯ÙŠØŸ Ø­ÙƒÙŠÙ„ÙŠ Ø¹Ù† Ø§Ù„Ø°ÙƒØ±Ù‰ Ø¯ÙŠ."
    
    def _get_mock_audio_response(self, context: str) -> Dict[str, Any]:
        """Mock audio analysis response"""
        return {
            "transcription": "Ø§Ù„Ù…Ø±ÙŠØ¶ ÙŠØªÙƒÙ„Ù… Ø¨ÙˆØ¶ÙˆØ­ ÙˆÙŠØ¨Ø¯Ùˆ Ù‡Ø§Ø¯Ø¦",
            "analysis": "Ø§Ù„ØµÙˆØª ÙˆØ§Ø¶Ø­ ÙˆØ§Ù„ÙƒÙ„Ø§Ù… Ù…ÙÙ‡ÙˆÙ…. Ø§Ù„Ù…Ø±ÙŠØ¶ ÙŠØ¨Ø¯Ùˆ ÙÙŠ Ø­Ø§Ù„Ø© Ø¬ÙŠØ¯Ø© Ø§Ù„ÙŠÙˆÙ….",
            "mood_indicators": ["calm", "clear"],
            "cognitive_markers": ["coherent_speech", "good_articulation"]
        }
