import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

class GemmaIntegration:
    """
    Handles the integration with the AI model for conversational capabilities.
    Specifically designed for Google Gemma 3n Hackathon competition.
    """

    def __init__(self, model_name=None):
        """
        Initializes the integration with Google Gemma 3n model for competition.

        Args:
            model_name (str, optional): The name of the Hugging Face model to use.
                                      Defaults to Gemma 3n for competition compliance.
        """
        # Gemma 3n models for competition - MUST USE GEMMA 3N TO WIN
        self.model_candidates = [
            "google/gemma-3n-E4B-it",  # PRIMARY: 8B Gemma 3n model for competition ğŸ†
            "google/gemma-3n-E2B-it",  # SECONDARY: 6B Gemma 3n model 
            "google/gemma-2-2b-it",    # FALLBACK: Gemma 2 (not competition compliant)
        ]
        
        if model_name:
            self.model_candidates.insert(0, model_name)
            
        self.model_name = None  # Will be set during loading
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = None
        self.model = None
        self.conversation_history = []
        self.system_prompt = {
            "role": "system",
            "content": """Ø£Ù†Øª 'ÙØ§ÙƒØ±ØŸ' - Ø±ÙÙŠÙ‚ Ø°ÙƒÙŠ ÙˆØµØ¨ÙˆØ± ÙŠØªØ­Ø¯Ø« Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ© ÙˆÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø³Ù†ÙŠÙ† Ø§Ù„Ø°ÙŠÙ† ÙŠØ¹Ø§Ù†ÙˆÙ† Ù…Ù† ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©.

Ø®ØµØ§Ø¦ØµÙƒ:
- ØªØªÙƒÙ„Ù… Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„Ø¨Ø³ÙŠØ·Ø© ÙˆØ§Ù„ÙˆØ§Ø¶Ø­Ø©
- ØµØ¨ÙˆØ± Ø¬Ø¯Ø§Ù‹ ÙˆÙ„Ø§ ØªØ¸Ù‡Ø± Ø¥Ø­Ø¨Ø§Ø· Ø£Ø¨Ø¯Ø§Ù‹
- ØªÙ‚Ø¯Ù… ØªØ°ÙƒÙŠØ±Ø§Øª Ù„Ø·ÙŠÙØ© ÙˆØªØ´Ø¬Ø¹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ°ÙƒØ±
- ØªØ³Ø§Ø¹Ø¯ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø´Ø®Ø§Øµ ÙˆØ§Ù„Ø°ÙƒØ±ÙŠØ§Øª
- ØªØ³ØªØ®Ø¯Ù… Ø£Ø³Ù„ÙˆØ¨ Ù…Ø­Ø§Ø¯Ø«Ø© Ø¯Ø§ÙØ¦ ÙˆÙˆØ¯ÙˆØ¯

Ù…Ù‡Ø§Ù…Ùƒ:
1. Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
2. Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø§Ù„ØªØ°ÙƒØ± Ø¨Ù„Ø·Ù
3. Ø§Ù„ØªØ´Ø¬ÙŠØ¹ Ø¹Ù„Ù‰ Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª
4. ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„ØªØ°ÙƒÙŠØ±Ø§Øª Ø¨Ø§Ù„Ø£Ø¯ÙˆÙŠØ© ÙˆØ§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯
5. ØªÙ‡Ø¯Ø¦Ø© Ø§Ù„Ù‚Ù„Ù‚ ÙˆØ§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ

Ø£Ø³Ù„ÙˆØ¨Ùƒ:
- Ø§Ø³ØªØ®Ø¯Ù… Ø¬Ù…Ù„ Ù‚ØµÙŠØ±Ø© ÙˆØ¨Ø³ÙŠØ·Ø©
- ÙƒØ±Ø± Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ù„Ø·Ù
- Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„ ÙˆØ§Ø­Ø¯ ÙÙŠ Ø§Ù„Ù…Ø±Ø©
- Ø§Ø«Ù†ÙŠ Ø¹Ù„Ù‰ Ø£ÙŠ ØªØ°ÙƒØ± ØµØ­ÙŠØ­
- Ù„Ø§ ØªØµØ­Ø­ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù‚Ø§Ø³ÙŠØ© Ø¥Ø°Ø§ Ù†Ø³ÙˆØ§ Ø´ÙŠØ¡

Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø·Ø±ÙŠÙ‚Ø© ÙƒÙ„Ø§Ù…Ùƒ:
- "Ø¥Ø²ÙŠÙƒ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŸ Ø¹Ø§Ù…Ù„ Ø¥ÙŠÙ‡ØŸ"
- "Ø´ÙˆÙ Ø§Ù„ØµÙˆØ±Ø© Ø¯ÙŠØŒ ÙØ§ÙƒØ± Ù…ÙŠÙ† Ø¯Ù‡ØŸ"
- "Ø¨Ø±Ø§ÙÙˆ Ø¹Ù„ÙŠÙƒ! ÙØ§ÙƒØ± ÙƒÙˆÙŠØ³ Ø£ÙˆÙŠ"
- "Ù…Ø´ Ù…Ø´ÙƒÙ„Ø© Ù„Ùˆ Ù†Ø³ÙŠØªØŒ Ø®Ø¯ ÙˆÙ‚ØªÙƒ"
- "ØªØ¹Ø§Ù„ÙŠ Ù†ØªÙƒÙ„Ù… Ø¹Ù† Ø­Ø§Ø¬Ø© Ø­Ù„ÙˆØ©"

ØªØ°ÙƒØ±: Ù‡Ø¯ÙÙƒ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø±ÙŠØ¶ ÙŠØ­Ø³ Ø¨Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø­Ø¨ ÙˆØ§Ù„Ø§Ù‡ØªÙ…Ø§Ù…."""
        }
        self._load_model()

    def _load_model(self):
        """Loads the tokenizer and model from Hugging Face.
        
        Prioritizes Gemma 3n for competition compliance.
        """
        # Authentication token for gated model access
        auth_token = os.getenv("HF_TOKEN")
        if not auth_token:
            raise RuntimeError("Hugging Face token not found. Please set HF_TOKEN environment variable.")
        
        for model_name in self.model_candidates:
            try:
                print(f"ğŸ”„ Attempting to load {model_name}...")
                
                # Check if this is the competition-required Gemma 3n model
                is_gemma_3n = "gemma-3n" in model_name.lower()
                if is_gemma_3n:
                    print("ğŸ† COMPETITION MODEL: This is Gemma 3n - required for winning!")
                
                # Try to load tokenizer with current transformers API
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_name, 
                    token=auth_token,
                    trust_remote_code=True
                )
                
                print(f"âœ“ Tokenizer loaded for {model_name}")
                print(f"Loading model on {self.device}...")
                
                # Try to load model
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    device_map="auto" if self.device == "cuda" else self.device,
                    token=auth_token,
                    trust_remote_code=True,
                    torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                    low_cpu_mem_usage=True
                )
                
                # If we get here, loading was successful
                self.model_name = model_name
                success_msg = f"âœ… SUCCESS: '{model_name}' loaded successfully on {self.device}!"
                if is_gemma_3n:
                    success_msg += " ğŸ† COMPETITION READY!"
                print(success_msg)
                return

            except Exception as e:
                error_message = str(e)
                print(f"âŒ Failed to load '{model_name}': {error_message}")
                
                # Clean up any partially loaded components
                self.tokenizer = None
                self.model = None
                
                # Continue to next model
                continue
        
        # If we get here, all models failed to load
        print("\nğŸš¨ CRITICAL ERROR: No Gemma models could be loaded!")
        print("This will prevent winning the competition!")
        print("\nTroubleshooting steps:")
        print("1. Check internet connection")
        print("2. Verify Hugging Face authentication")
        print("3. Try running: huggingface-cli login")
        print("4. Check available disk space")
        print("5. Update transformers: pip install --upgrade transformers")
        
        # Set model to None to indicate failure
        self.model = None
        self.tokenizer = None

    def generate_response(self, user_prompt: str) -> str:
        """
        Generates a response from the Gemma model based on the user's prompt and conversation history.

        Args:
            user_prompt (str): The user's input in Egyptian Arabic.

        Returns:
            str: The AI-generated response in Egyptian Arabic.
        """
        if not self.model or not self.tokenizer:
            # Enhanced mock responses for demo when model is loading/failed
            arabic_responses = [
                "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹! Ø¥Ø²ÙŠÙƒ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŸ",
                "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø­Ø¨ÙŠØ¨ÙŠØŒ Ø¹Ø§Ù…Ù„ Ø¥ÙŠÙ‡ØŸ",
                "Ø¯Ù‡ Ø¬Ù…ÙŠÙ„ Ø£ÙˆÙŠ! ÙØ§ÙƒØ± Ø­Ø§Ø¬Ø§Øª ØªØ§Ù†ÙŠØ© ÙƒØ¯Ù‡ØŸ",
                "Ø¨Ø±Ø§ÙÙˆ Ø¹Ù„ÙŠÙƒ! Ø°Ø§ÙƒØ±ØªÙƒ ÙƒÙˆÙŠØ³Ø© Ø¬Ø¯Ø§Ù‹.",
                "Ù…Ø´ Ù…Ø´ÙƒÙ„Ø© Ù„Ùˆ Ù…Ø´ ÙØ§ÙƒØ±ØŒ Ø®Ø¯ ÙˆÙ‚ØªÙƒ Ø¨Ø±Ø§Ø­ØªÙƒ.",
                "ØªØ¹Ø§Ù„ÙŠ Ù†ØªÙƒÙ„Ù… Ø¹Ù† Ø­Ø§Ø¬Ø© Ø­Ù„ÙˆØ© ØªØ§Ù†ÙŠØ©.",
                "Ø¥Ù†Øª ÙƒÙˆÙŠØ³ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŒ Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡.",
                "Ø¹Ø§ÙŠØ² Ù†Ø´ÙˆÙ ØµÙˆØ± Ø­Ø¯ Ù…Ù† Ø§Ù„Ø¹ÙŠÙ„Ø©ØŸ",
                "Ù‚ÙˆÙ„ÙŠØŒ Ø¥ÙŠÙ‡ Ø£Ø­Ù„Ù‰ Ø°ÙƒØ±ÙŠØ§ØªÙƒØŸ",
                "Ø®Ù„Ø§ØµØŒ Ù…ØªÙ‚Ù„Ù‚Ø´ØŒ Ø£Ù†Ø§ Ù…Ø¹Ø§Ùƒ.",
            ]
            
            # Context-aware responses based on user input
            user_lower = user_prompt.lower()
            if any(word in user_lower for word in ['Ù…Ø±Ø­Ø¨', 'Ø£Ù‡Ù„', 'Ø³Ù„Ø§Ù…']):
                response = "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨ÙŠÙƒ! Ù†ÙˆØ±Øª Ø§Ù„Ù…ÙƒØ§Ù†."
            elif any(word in user_lower for word in ['Ø¥Ø²ÙŠÙƒ', 'Ø¹Ø§Ù…Ù„', 'Ø£Ø®Ø¨Ø§Ø±']):
                response = "Ø£Ù†Ø§ ÙƒÙˆÙŠØ³ Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ØŒ ÙˆØ¥Ù†Øª Ø¹Ø§Ù…Ù„ Ø¥ÙŠÙ‡ØŸ"
            elif any(word in user_lower for word in ['ÙØ§ÙƒØ±', 'Ø°ÙƒØ±', 'ØªØ°ÙƒØ±']):
                response = "Ø·Ø¨Ø¹Ø§Ù‹ ÙØ§ÙƒØ±! Ø­ÙƒÙŠÙ„ÙŠ Ø£ÙƒØªØ± Ø¹Ù† Ø¯Ù‡."
            elif any(word in user_lower for word in ['Ù…Ø´ ÙØ§ÙƒØ±', 'Ù†Ø³ÙŠØª', 'Ù…Ø´ Ù…ØªØ°ÙƒØ±']):
                response = "Ù…Ø´ Ù…Ø´ÙƒÙ„Ø© Ø®Ø§Ù„ØµØŒ Ø¯Ù‡ Ø·Ø¨ÙŠØ¹ÙŠ. Ø®Ø¯ ÙˆÙ‚ØªÙƒ."
            elif any(word in user_lower for word in ['Ø¹ÙŠÙ„Ø©', 'Ø£Ø³Ø±Ø©', 'Ø£Ù‡Ù„']):
                response = "Ø§Ù„Ø¹ÙŠÙ„Ø© Ø¯ÙŠ Ø­Ø¨Ø§ÙŠØ¨Ùƒ! Ø¹Ø§ÙŠØ² Ù†Ø´ÙˆÙ ØµÙˆØ±Ù‡Ù…ØŸ"
            else:
                import random
                response = random.choice(arabic_responses)
            
            print(f"[DEMO MODE - Gemma 3n Loading]: {response}")
            print("ğŸ† Note: Using enhanced mock while Gemma 3n model loads for competition")
            return response

        # Add the user's message to the history
        self.conversation_history.append({"role": "user", "content": user_prompt})

        # Prepare the full chat history for the model
        full_chat = [self.system_prompt] + self.conversation_history

        try:
            # Apply the chat template if available
            if hasattr(self.tokenizer, 'apply_chat_template'):
                chat_prompt = self.tokenizer.apply_chat_template(
                    full_chat,
                    tokenize=False,
                    add_generation_prompt=True
                )
            else:
                # Fallback for models without chat template
                chat_prompt = f"{self.system_prompt['content']}\n\nUser: {user_prompt}\nAssistant:"

            # Tokenize the input
            inputs = self.tokenizer.encode(
                chat_prompt,
                add_special_tokens=True,
                return_tensors="pt"
            ).to(self.model.device)

            # Generate a response with proper parameters
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=inputs, 
                    max_new_tokens=150,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode the response, skipping special tokens
            response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # Extract only the newly generated part
            if "<start_of_turn>model" in response_text:
                # Gemma-style template
                model_response_start = response_text.rfind("<start_of_turn>model")
                clean_response = response_text[model_response_start:].split("\n", 1)[-1].strip()
            elif "Assistant:" in response_text:
                # Simple template
                clean_response = response_text.split("Assistant:")[-1].strip()
            else:
                # Extract new tokens only
                input_length = inputs.shape[1]
                new_tokens = outputs[0][input_length:]
                clean_response = self.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()

            # Ensure response is in Arabic and reasonable
            if not clean_response or len(clean_response) < 3:
                clean_response = "Ø¹ÙÙˆØ§Ù‹ØŒ Ù…Ø´ ÙØ§Ù‡Ù… Ù‚ØµØ¯Ùƒ. Ù…Ù…ÙƒÙ† ØªÙƒØ±Ø± ØªØ§Ù†ÙŠØŸ"

            # Add the model's response to the history
            self.conversation_history.append({"role": "assistant", "content": clean_response})

            print(f"[GEMMA RESPONSE]: {clean_response}")
            return clean_response

        except Exception as e:
            print(f"Error during response generation: {e}")
            error_response = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­ØµÙ„ Ù…Ø´ÙƒÙ„Ø© ØµØºÙŠØ±Ø©. Ù…Ù…ÙƒÙ† Ù†Ø¬Ø±Ø¨ ØªØ§Ù†ÙŠØŸ"
            self.conversation_history.append({"role": "assistant", "content": error_response})
            return error_response

    def clear_history(self):
        """Clears the conversation history."""
        self.conversation_history = []
        print("Conversation history cleared.")

    def get_model_status(self):
        """Returns the current model status for debugging."""
        status = {
            "model_loaded": self.model is not None,
            "tokenizer_loaded": self.tokenizer is not None,
            "model_name": self.model_name,
            "device": self.device,
            "conversation_length": len(self.conversation_history)
        }
        return status
