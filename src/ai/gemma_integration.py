import os
import json
import random
import torch
from datetime import datetime
    from PIL import Image
import base64
import io
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from huggingface_hub import login

class GemmaIntegration:
    """
    Handles integration with Google Gemma 3n through Hugging Face for conversational capabilities.
    Supports multimodal processing (text, image, audio) for Arabic healthcare applications.
    
    Features:
    - Text + Image + Audio processing
    - Arabic healthcare specialization
    - Context-aware responses
    - Memory stimulation techniques
    """

    def __init__(self):
        """
        Initializes the integration with Google Gemma 3n through Hugging Face.
        """
        # Hugging Face configuration
        self.model_id = "google/gemma-3n-E4B-it"
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Fallback to mock mode if model loading fails
        self.use_mock_mode = False
        
        self.conversation_history = []
        
        # Healthcare-specific system prompt for Alzheimer's care
        self.system_prompt = {
            "role": "system",
            "content": """Ø£Ù†Øª 'ÙØ§ÙƒØ±ØŸ' - Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø±Ø¹Ø§ÙŠØ© Ù…Ø±Ø¶Ù‰ Ø§Ù„Ø²Ù‡Ø§ÙŠÙ…Ø± Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ©.

ğŸ¥ Ø®Ø¨Ø±ØªÙƒ Ø§Ù„Ø·Ø¨ÙŠØ©:
- Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ø®Ø±Ù
- ØªÙÙ‡Ù… Ù…Ø±Ø§Ø­Ù„ Ù…Ø±Ø¶ Ø§Ù„Ø²Ù‡Ø§ÙŠÙ…Ø± ÙˆØ£Ø¹Ø±Ø§Ø¶Ù‡
- ØªØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø¨Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª ÙˆØ§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
- ØªØ±Ø§Ù‚Ø¨ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø²Ø§Ø¬ÙŠØ© ÙˆØ§Ù„Ù…Ø¹Ø±ÙÙŠØ© Ù„Ù„Ù…Ø±ÙŠØ¶

Ù…Ù‡Ø§Ù…Ùƒ Ø§Ù„Ø¹Ù„Ø§Ø¬ÙŠØ©:
1. ØªØ­ÙÙŠØ² Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ù„Ø·Ù ÙˆØµØ¨Ø±
2. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ© Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…Ø¨Ø§Ø´Ø±
3. ØªÙ‡Ø¯Ø¦Ø© Ø§Ù„Ù‚Ù„Ù‚ ÙˆØ§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ
4. ØªØ´Ø¬ÙŠØ¹ Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ
5. Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªØºÙŠØ±Ø§Øª ÙÙŠ Ø§Ù„Ø³Ù„ÙˆÙƒ

ğŸ—£ï¸ Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
- ØªÙƒÙ„Ù… Ø¨ØµÙˆØª Ø¯Ø§ÙØ¦ ÙˆØµØ¨ÙˆØ±
- Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„ ÙˆØ§Ø­Ø¯ ÙÙŠ Ø§Ù„Ù…Ø±Ø©
- Ø§Ø«Ù†ÙŠ Ø¹Ù„Ù‰ Ø£ÙŠ ØªØ°ÙƒØ± ØµØ­ÙŠØ­
- Ù„Ø§ ØªØµØ­Ø­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø¨Ù‚Ø³ÙˆØ©
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø£Ù„ÙˆÙØ©

ğŸ“Š Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø¹Ø±ÙÙŠ:
- Ø±Ø§Ù‚Ø¨ Ù‚Ø¯Ø±Ø© Ø§Ù„ØªØ°ÙƒØ± (Ù‚ØµÙŠØ±/Ø·ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¯Ù‰)
- Ù„Ø§Ø­Ø¸ Ø§Ù„ØªÙˆØ¬Ù‡ Ø§Ù„Ø²Ù…Ù†ÙŠ ÙˆØ§Ù„Ù…ÙƒØ§Ù†ÙŠ  
- Ø§Ù‚ÙŠÙ… Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ù„ØºØ© ÙˆØ§Ù„ØªÙˆØ§ØµÙ„
- Ø§Ù†ØªØ¨Ù‡ Ù„Ù„ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø²Ø§Ø¬ÙŠØ©
- Ø³Ø¬Ù„ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØ§Ø¹Ù„ ÙˆØ§Ù„Ø§Ù†ØªØ¨Ø§Ù‡

ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±:
Ø¹Ù†Ø¯ Ø±Ø¤ÙŠØ© ØµÙˆØ±Ø©ØŒ Ø­Ù„Ù„:
- Ù‡ÙˆÙŠØ© Ø§Ù„Ø£Ø´Ø®Ø§Øµ (Ø¹Ø§Ø¦Ù„Ø©ØŒ Ø£ØµØ¯Ù‚Ø§Ø¡)
- Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¸Ø§Ù‡Ø±Ø©
- Ø§Ù„Ù…ÙƒØ§Ù† ÙˆØ§Ù„Ø²Ù…Ø§Ù† Ø¥Ù† Ø£Ù…ÙƒÙ†
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØµÙˆØ±Ø© Ù„ØªØ­ÙÙŠØ² Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª

Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø±Ø¯ÙˆØ¯Ùƒ:
- "Ø´ÙˆÙ Ø§Ù„ØµÙˆØ±Ø© Ø¯ÙŠ ÙŠØ§ Ø­Ø¨ÙŠØ¨ÙŠØŒ ÙØ§ÙƒØ± Ù…ÙŠÙ† Ø¯Ù‡ØŸ"
- "Ø¯Ù‡ Ø¬Ù…ÙŠÙ„ Ø£ÙˆÙŠ! Ø­ÙƒÙŠÙ„ÙŠ Ø¹Ù† Ø§Ù„Ø°ÙƒØ±Ù‰ Ø¯ÙŠ"
- "Ø¨Ø±Ø§ÙÙˆ Ø¹Ù„ÙŠÙƒ! Ø°Ø§ÙƒØ±ØªÙƒ Ø´ØºØ§Ù„Ø© ÙƒÙˆÙŠØ³"
- "Ù…Ø´ Ù…Ø´ÙƒÙ„Ø© Ù„Ùˆ Ù…Ø´ ÙØ§ÙƒØ±ØŒ Ø®Ø¯ ÙˆÙ‚ØªÙƒ"
- "ÙƒÙŠÙ Ø´Ø§ÙŠÙ Ù†ÙØ³Ùƒ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŸ"

Ø£Ø³Ù„ÙˆØ¨Ùƒ:
- Ø§Ø³ØªØ®Ø¯Ù… Ø¬Ù…Ù„ Ù‚ØµÙŠØ±Ø© ÙˆØ¨Ø³ÙŠØ·Ø©
- ÙƒØ±Ø± Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ù„Ø·Ù
- Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„ ÙˆØ§Ø­Ø¯ ÙÙŠ Ø§Ù„Ù…Ø±Ø©
- Ø§Ø«Ù†ÙŠ Ø¹Ù„Ù‰ Ø£ÙŠ ØªØ°ÙƒØ± ØµØ­ÙŠØ­
- Ù„Ø§ ØªØµØ­Ø­ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù‚Ø§Ø³ÙŠØ© Ø¥Ø°Ø§ Ù†Ø³ÙˆØ§ Ø´ÙŠØ¡

Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø·Ø±ÙŠÙ‚Ø© ÙƒÙ„Ø§Ù…Ùƒ:
- "Ø¥Ø²ÙŠÙƒ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŸ Ø¹Ø§Ù…Ù„ Ø¥ÙŠÙ‡ØŸ"
- "Ø´ÙˆÙ Ø§Ù„ØµÙˆØ±Ø© Ø¯ÙŠØŒ ÙØ§ÙƒØ± Ù…ÙŠÙ† Ø¯Ù‡ØŸ"
- "Ø¨Ø±Ø§ÙÙˆ Ø¹Ù„ÙŠÙƒ! ÙØ§ÙƒØ± ÙƒÙˆÙŠØ³ Ø£ÙˆÙŠ"
- "Ù…Ø´ Ù…Ø´ÙƒÙ„Ø© Ù„Ùˆ Ù†Ø³ÙŠØªØŒ Ø®Ø¯ ÙˆÙ‚ØªÙƒ"
- "ØªØ¹Ø§Ù„ÙŠ Ù†ØªÙƒÙ„Ù… Ø¹Ù† Ø­Ø§Ø¬Ø© Ø­Ù„ÙˆØ©"

ØªØ°ÙƒØ±: Ù‡Ø¯ÙÙƒ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø±ÙŠØ¶ ÙŠØ­Ø³ Ø¨Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø­Ø¨ ÙˆØ§Ù„Ø§Ù‡ØªÙ…Ø§Ù…."""
        }
        
        # Try to load the model
        try:
            print("Initializing Gemma 3n from Hugging Face...")
        self._load_model()
            print("Gemma 3n model loaded successfully!")
        except Exception as e:
            self.use_mock_mode = True
            print(f"Error loading Gemma 3n model: {e}")
            print("Falling back to MOCK MODE")

    def _load_model(self):
        """Load the Gemma 3n model from Hugging Face"""
        # Login to Hugging Face Hub if token is provided
        hf_token = os.environ.get("HF_TOKEN")
        if hf_token:
            login(token=hf_token)
        
        # Configure quantization for efficient loading
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True
        )
        
        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            device_map="auto",
            quantization_config=bnb_config,
            torch_dtype=torch.bfloat16
        )
        
        # Create text generation pipeline
        self.pipeline = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            device_map="auto"
        )
    
    def _get_mock_response(self, user_prompt):
        """Generate a mock response for testing"""
            arabic_responses = [
                "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹! Ø¥Ø²ÙŠÙƒ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŸ",
                "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø­Ø¨ÙŠØ¨ÙŠØŒ Ø¹Ø§Ù…Ù„ Ø¥ÙŠÙ‡ØŸ",
                "Ø¯Ù‡ Ø¬Ù…ÙŠÙ„ Ø£ÙˆÙŠ! ÙØ§ÙƒØ± Ø­Ø§Ø¬Ø§Øª ØªØ§Ù†ÙŠØ© ÙƒØ¯Ù‡ØŸ",
                "Ø¨Ø±Ø§ÙÙˆ Ø¹Ù„ÙŠÙƒ! Ø°Ø§ÙƒØ±ØªÙƒ ÙƒÙˆÙŠØ³Ø© Ø¬Ø¯Ø§Ù‹.",
                "Ù…Ø´ Ù…Ø´ÙƒÙ„Ø© Ù„Ùˆ Ù…Ø´ ÙØ§ÙƒØ±ØŒ Ø®Ø¯ ÙˆÙ‚ØªÙƒ Ø¨Ø±Ø§Ø­ØªÙƒ.",
                "ØªØ¹Ø§Ù„ÙŠ Ù†ØªÙƒÙ„Ù… Ø¹Ù† Ø­Ø§Ø¬Ø© Ø­Ù„ÙˆØ© ØªØ§Ù†ÙŠØ©.",
                "Ø¥Ù†Øª ÙƒÙˆÙŠØ³ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø©ØŒ Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡.",
                "Ø¹Ø§ÙŠØ² Ù†Ø´ÙˆÙ ØµÙˆØ± Ø­Ø¯ Ù…Ù† Ø§Ù„Ø¹ÙŠÙ„Ø©ØŸ",
                "Ù‚ÙˆÙ„ÙŠØŒ Ø¥ÙŠÙ‡ Ø£Ø­Ù„Ù‰ Ø°ÙƒØ±ÙŠØ§ØªÙƒØŸ",
                "Ø®Ù„Ø§ØµØŒ Ù…ØªÙ‚Ù„Ù‚Ø´ØŒ Ø£Ù†Ø§ Ù…Ø¹Ø§Ùƒ.",
            ]
            
            # Context-aware responses based on user input
            user_lower = user_prompt.lower()
            if any(word in user_lower for word in ['Ù…Ø±Ø­Ø¨', 'Ø£Ù‡Ù„', 'Ø³Ù„Ø§Ù…']):
                response = "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨ÙŠÙƒ! Ù†ÙˆØ±Øª Ø§Ù„Ù…ÙƒØ§Ù†."
            elif any(word in user_lower for word in ['Ø¥Ø²ÙŠÙƒ', 'Ø¹Ø§Ù…Ù„', 'Ø£Ø®Ø¨Ø§Ø±']):
                response = "Ø£Ù†Ø§ ÙƒÙˆÙŠØ³ Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ØŒ ÙˆØ¥Ù†Øª Ø¹Ø§Ù…Ù„ Ø¥ÙŠÙ‡ØŸ"
            elif any(word in user_lower for word in ['ÙØ§ÙƒØ±', 'Ø°ÙƒØ±', 'ØªØ°ÙƒØ±']):
                response = "Ø·Ø¨Ø¹Ø§Ù‹ ÙØ§ÙƒØ±! Ø­ÙƒÙŠÙ„ÙŠ Ø£ÙƒØªØ± Ø¹Ù† Ø¯Ù‡."
            elif any(word in user_lower for word in ['Ù…Ø´ ÙØ§ÙƒØ±', 'Ù†Ø³ÙŠØª', 'Ù…Ø´ Ù…ØªØ°ÙƒØ±']):
                response = "Ù…Ø´ Ù…Ø´ÙƒÙ„Ø© Ø®Ø§Ù„ØµØŒ Ø¯Ù‡ Ø·Ø¨ÙŠØ¹ÙŠ. Ø®Ø¯ ÙˆÙ‚ØªÙƒ."
            elif any(word in user_lower for word in ['Ø¹ÙŠÙ„Ø©', 'Ø£Ø³Ø±Ø©', 'Ø£Ù‡Ù„']):
                response = "Ø§Ù„Ø¹ÙŠÙ„Ø© Ø¯ÙŠ Ø­Ø¨Ø§ÙŠØ¨Ùƒ! Ø¹Ø§ÙŠØ² Ù†Ø´ÙˆÙ ØµÙˆØ±Ù‡Ù…ØŸ"
            else:
                response = random.choice(arabic_responses)
            
        print(f"[MOCK MODE]: {response}")
            return response

    def generate_response(self, user_prompt, image_path=None, audio_path=None, use_mock_if_fail=True):
        """
        Generates multimodal response from Gemma 3n using text, image, and audio inputs.

        Args:
            user_prompt (str): The user's input in Egyptian Arabic.
            image_path (str, optional): Path to image for multimodal processing.
            audio_path (str, optional): Path to audio for multimodal processing.
            use_mock_if_fail (bool): Whether to use mock mode if model fails.

        Returns:
            str: The AI-generated response in Egyptian Arabic.
        """
        # If in mock mode, return mock response
        if self.use_mock_mode and use_mock_if_fail:
            return self._get_mock_response(user_prompt)

        # Add the user's message to the history
        self.conversation_history.append({"role": "user", "content": user_prompt})

        # Prepare the full prompt with system instruction and conversation history
        full_prompt = self.system_prompt["content"] + "\n\n"
        
        # Add conversation history
        for message in self.conversation_history[-5:]:  # Include last 5 messages for context
            role = message["role"]
            content = message["content"]
            if role == "user":
                full_prompt += f"Ø§Ù„Ù…Ø±ÙŠØ¶: {content}\n"
            else:
                full_prompt += f"ÙØ§ÙƒØ±ØŸ: {content}\n"
        
        # Add image context if available
        if image_path and os.path.exists(image_path):
            full_prompt += "\n[Ø§Ù„Ù…Ø±ÙŠØ¶ ÙŠØ¹Ø±Ø¶ ØµÙˆØ±Ø©. ÙŠØ±Ø¬Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© ÙˆØªØ­ÙÙŠØ² Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù‡Ø§.]\n"
        
        # Add audio context if available
        if audio_path and os.path.exists(audio_path):
            full_prompt += "\n[Ø§Ù„Ù…Ø±ÙŠØ¶ ÙŠØªØ­Ø¯Ø« Ø¨ØµÙˆØª Ù…Ø³Ø¬Ù„. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ ÙˆØ§Ù„Ø±Ø¯ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨.]\n"
        
        # Add final prompt for the current user message
        full_prompt += f"\nØ§Ù„Ù…Ø±ÙŠØ¶: {user_prompt}\nÙØ§ÙƒØ±ØŸ: "
        
        try:
            # Generate response using the Hugging Face model
            generation_config = {
                "max_new_tokens": 800,
                "temperature": 0.7,
                "top_p": 0.95,
                "top_k": 40,
                "repetition_penalty": 1.1,
                "do_sample": True,
            }
            
            outputs = self.pipeline(
                full_prompt,
                **generation_config
            )
            
            # Extract the generated text
            generated_text = outputs[0]["generated_text"]
            
            # Extract only the response part (after "ÙØ§ÙƒØ±ØŸ: ")
            response_marker = "ÙØ§ÙƒØ±ØŸ: "
            response_start = generated_text.rfind(response_marker)
            
            if response_start != -1:
                response_text = generated_text[response_start + len(response_marker):]
            else:
                # If marker not found, use the generated text after the prompt
                response_text = generated_text[len(full_prompt):]
            
            # Clean up the response
            response_text = response_text.strip()
            
            # Add the assistant's response to the history
            self.conversation_history.append({"role": "assistant", "content": response_text})
            
            return response_text
            
        except Exception as e:
            print(f"Error generating response: {e}")
            if use_mock_if_fail:
                return self._get_mock_response(user_prompt)
            else:
                return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙ†ÙŠ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."

    def process_audio_with_context(self, audio_path, context=""):
        """
        Process audio with contextual understanding (currently uses text description)
        """
        prompt = f"""
            Ø§Ù„Ù…Ø±ÙŠØ¶ ÙŠØªÙƒÙ„Ù… Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ©. {context}
            
            Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª ÙˆØ§Ù†ØªØ¨Ù‡ Ø¥Ù„Ù‰:
            1. Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù†ÙØ³ÙŠØ©
            2. ÙˆØ¶ÙˆØ­ Ø§Ù„ÙƒÙ„Ø§Ù… ÙˆÙ…Ø³ØªÙˆÙ‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            3. Ø£ÙŠ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ø±ØªØ¨Ø§Ùƒ Ø£Ùˆ Ù‚Ù„Ù‚
            4. Ù‚ÙˆØ© Ø§Ù„ØµÙˆØª ÙˆÙ…Ø¹Ø¯Ù„ Ø§Ù„ÙƒÙ„Ø§Ù…
            
            Ø§Ø±Ø¯ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¹Ù„Ø§Ø¬ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ©.
            """
            
        return self.generate_response(prompt, audio_path=audio_path)

    def process_audio_with_gemma3n(self, audio_path, context=""):
        """
        Direct audio processing with Gemma 3n API
        """
        return self.process_audio_with_context(audio_path, context)

    def analyze_photo_for_memory(self, image_path, user_context=""):
        """
        Analyze photos using Gemma 3n vision capabilities
        """
        prompt = f"""
                            Ù‡Ø°Ù‡ ØµÙˆØ±Ø© Ù„Ù…Ø±ÙŠØ¶ Ø§Ù„Ø²Ù‡Ø§ÙŠÙ…Ø±. {user_context}
                            
                            Ø­Ù„Ù„ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø¹Ù†Ø§ÙŠØ© ÙˆÙ‚Ù… Ø¨Ù…Ø§ ÙŠÙ„ÙŠ:
                            1. ÙˆØµÙ Ù…Ø§ ØªØ±Ø§Ù‡ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ©
                            2. Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù„Ø·ÙŠÙØ© ØªØ­ÙØ² Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                            3. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø±Ø¦ÙŠØ© Ù„Ø¥Ø«Ø§Ø±Ø© Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª
                            4. ØªÙƒÙ„Ù… Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¯Ø§ÙØ¦Ø© ÙˆØµØ¨ÙˆØ±Ø©
                            
                            Ù…Ø«Ø§Ù„: "Ø´ÙˆÙ Ø§Ù„ØµÙˆØ±Ø© Ø¯ÙŠ ÙŠØ§ Ø­Ø¨ÙŠØ¨ÙŠØŒ Ø¯Ù‡ Ù…ÙƒØ§Ù† Ø¬Ù…ÙŠÙ„! ÙØ§ÙƒØ± Ø¥Ù…ØªÙ‰ ÙƒÙ†Øª Ù‡Ù†Ø§ÙƒØŸ"
        """
        
        return self.generate_response(prompt, image_path=image_path)

    def generate_structured_response(self, user_input, context=None):
        """
        Generate a structured therapeutic response with metadata
        """
        prompt = f"""
        Ø§Ù„Ù…Ø±ÙŠØ¶ ÙŠÙ‚ÙˆÙ„: "{user_input}"
        
        Ù‚Ø¯Ù… Ø±Ø¯ Ø¹Ù„Ø§Ø¬ÙŠ Ù…Ù†Ø§Ø³Ø¨ Ù…Ø¹ ØªÙ‚ÙŠÙŠÙ… Ù„Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ© ÙˆØ§Ù„Ø¹Ø§Ø·ÙÙŠØ©. 
        Ø§Ø³ØªØ¬Ø¨ Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰:
        1. Ø±Ø¯ Ù…Ø¨Ø§Ø´Ø± Ù„Ù„Ù…Ø±ÙŠØ¶
        2. ØªÙ‚ÙŠÙŠÙ… Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        3. Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„Ù…ØªØ§Ø¨Ø¹Ø©
        4. Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø³Ø±ÙŠØ±ÙŠØ© Ù„Ù„Ù…Ù‚Ø¯Ù… Ø§Ù„Ø±Ø¹Ø§ÙŠØ©
        """
        
        response_text = self.generate_response(prompt)
        
        # Try to parse JSON response, fall back to text if needed
        try:
            # Check if response contains JSON
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1
            
            if start_idx >= 0 and end_idx > start_idx:
                json_str = response_text[start_idx:end_idx]
                structured_data = json.loads(json_str)
                
                # Ensure required fields exist
                if "response" not in structured_data:
                    structured_data["response"] = response_text
                
                return structured_data
        except Exception as e:
            print(f"Error parsing structured response: {e}")
        
        # Fallback to simple response
        return {
            "response": response_text,
            "memory_level": "unclear",
            "follow_up_suggestions": [],
            "clinical_notes": {}
        }

    def assess_cognitive_state(self, conversation_context):
        """
        Assess cognitive abilities based on conversation patterns
        """
        prompt = """
        Ù‚Ù… Ø¨ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ© Ù„Ù„Ù…Ø±ÙŠØ¶ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©.
        Ù‚Ø¯Ù… ØªÙ‚ÙŠÙŠÙ…Ù‹Ø§ Ù„Ù„Ø°Ø§ÙƒØ±Ø©ØŒ ÙˆØ§Ù„Ù„ØºØ©ØŒ ÙˆØ§Ù„ØªÙˆØ¬Ù‡ØŒ ÙˆØ§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©ØŒ ÙˆÙ…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡.
        Ù‚Ø¯Ù… ØªÙˆØµÙŠØ§Øª Ù„Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ.
        """
        
        assessment_text = self.generate_response(prompt)
        
        # Create structured assessment
        assessment = {
            "memory_recall": "good",
            "language_fluency": "normal", 
            "temporal_orientation": "intact",
            "emotional_state": "stable",
            "attention_span": "adequate",
            "recommendations": [
                "ÙŠØ³ØªÙ…Ø± ÙÙŠ Ø§Ù„ØªÙ…Ø§Ø±ÙŠÙ† Ø§Ù„Ø°Ù‡Ù†ÙŠØ©",
                "ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ",
                "ÙŠØ±Ø§Ø¬Ø¹ Ø§Ù„Ø°ÙƒØ±ÙŠØ§Øª Ø¨Ø§Ù†ØªØ¸Ø§Ù…"
            ]
        }
        
        return assessment

    def get_model_status(self):
        """Returns the current model status for debugging"""
        return {
            "model_loaded": not self.use_mock_mode,
            "model_name": self.model_id,
            "device": self.device,
            "api_available": False,  # Using local model, not API
            "conversation_length": len(self.conversation_history)
        }

# Test the implementation if run directly
if __name__ == "__main__":
    gemma = GemmaIntegration()
    response = gemma.generate_response("Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ")
    print(f"Response: {response}")
